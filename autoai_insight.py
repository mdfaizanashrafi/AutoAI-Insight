# -*- coding: utf-8 -*-
"""AutoAI-Insight.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1lIXeVvfft99pfbBzw-3C6EFZBQEqpRR-
"""

import os
import sys

# Define base path
project_root = '/content/autoai-insight'

# Create folder structure
os.makedirs(project_root, exist_ok=True)
os.makedirs(f'{project_root}/backend/data_pipeline', exist_ok=True)
os.makedirs(f'{project_root}/data/raw', exist_ok=True)
os.makedirs(f'{project_root}/data/processed', exist_ok=True)

# Add project root to Python path for imports
sys.path.append(project_root)

print("âœ… Folder structure created successfully.")

# Write __init__.py
with open(f'{project_root}/backend/data_pipeline/__init__.py', 'w') as f:
    f.write('# AutoAI Insight Data Pipeline Module\n')

# Write data_loader.py
with open(f'{project_root}/backend/data_pipeline/data_loader.py', 'w') as f:
    f.write('''import pandas as pd
import polars as pl

def load_data(file_path: str, engine: str = 'pandas'):
    """
    Load data using pandas or polars.
    Supports CSV and JSON formats.
    """
    if file_path.endswith('.csv'):
        if engine == 'pandas':
            return pd.read_csv(file_path)
        elif engine == 'polars':
            return pl.read_csv(file_path).to_pandas()
    elif file_path.endswith('.json'):
        if engine == 'pandas':
            return pd.read_json(file_path)
        elif engine == 'polars':
            return pl.read_json(file_path).to_pandas()
    else:
        raise ValueError("Unsupported file format. Use .csv or .json")
''')

# Write utils.py
with open(f'{project_root}/backend/data_pipeline/utils.py', 'w') as f:
    f.write('''import pandas as pd

def detect_column_types(df: pd.DataFrame):
    """
    Detect column types: numerical, categorical, datetime
    """
    type_info = {}
    for col in df.columns:
        if pd.api.types.is_numeric_dtype(df[col]):
            type_info[col] = 'numerical'
        elif pd.api.types.is_datetime64_any_dtype(df[col]):
            type_info[col] = 'datetime'
        else:
            unique_ratio = df[col].nunique() / len(df[col])
            if unique_ratio < 0.05:
                type_info[col] = 'categorical'
            else:
                type_info[col] = 'text'
    return type_info
''')

# Write transformers.py
with open(f'{project_root}/backend/data_pipeline/transformers.py', 'w') as f:
    f.write('''from sklearn.base import TransformerMixin, BaseEstimator
import pandas as pd
import numpy as np

class OutlierHandler(BaseEstimator, TransformerMixin):
    def __init__(self, factor=1.5):
        self.factor = factor
        self.bounds_ = {}

    def fit(self, X, y=None):
        for col in X.select_dtypes(include=np.number).columns:
            q1 = X[col].quantile(0.25)
            q3 = X[col].quantile(0.75)
            iqr = q3 - q1
            self.bounds_[col] = (q1 - self.factor * iqr, q3 + self.factor * iqr)
        return self

    def transform(self, X):
        X = X.copy()
        for col, (lower, upper) in self.bounds_.items():
            if col in X.columns:
                X[col] = X[col].clip(lower=lower, upper=upper)
        return X


class DataFrameImputer(BaseEstimator, TransformerMixin):
    def __init__(self, numerical_strategy='median', categorical_strategy='mode'):
        self.numerical_strategy = numerical_strategy
        self.categorical_strategy = categorical_strategy
        self.fill_values_ = {}

    def fit(self, X, y=None):
        for col in X.columns:
            if pd.api.types.is_numeric_dtype(X[col]):
                if self.numerical_strategy == 'mean':
                    self.fill_values_[col] = X[col].mean()
                elif self.numerical_strategy == 'median':
                    self.fill_values_[col] = X[col].median()
            else:
                mode = X[col].mode()[0] if not X[col].mode().empty else None
                self.fill_values_[col] = mode
        return self

    def transform(self, X):
        return X.fillna(self.fill_values_)
''')

# Write preprocessing.py
with open(f'{project_root}/backend/data_pipeline/preprocessing.py', 'w') as f:
  f.write('''import pandas as pd
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from .transformers import OutlierHandler, DataFrameImputer
from .utils import detect_column_types

class DataPreprocessor:
    def __init__(self, config=None):
        self.config = config or {
            'numerical_strategy': 'median',
            'categorical_strategy': 'mode',
            'scale_numerical': True,
            'encode_categorical': True,
            'detect_outliers': True
        }

    def fit_transform(self, file_path, engine='pandas'):
        from .data_loader import load_data
        self.df_raw = load_data(file_path, engine=engine)
        self.type_info = detect_column_types(self.df_raw)

        numerical_cols = [k for k, v in self.type_info.items() if v == 'numerical']
        categorical_cols = [k for k, v in self.type_info.items() if v in ['categorical', 'text']]

        preprocessor_steps = []

        # Impute Numerical
        preprocessor_steps.append(('imputer', DataFrameImputer(
            numerical_strategy=self.config['numerical_strategy'],
            categorical_strategy=self.config['categorical_strategy']
        )))

        # Handle Outliers
        if self.config['detect_outliers']:
            preprocessor_steps.append(('outlier_handler', OutlierHandler()))

        # Scale Numerical
        if self.config['scale_numerical']:
            preprocessor_steps.append(('scaler', StandardScaler()))

        numerical_pipeline = Pipeline(preprocessor_steps)

        # Encode Categorical
        encoder = OneHotEncoder(handle_unknown='ignore') if self.config['encode_categorical'] else 'passthrough'

        full_preprocessor = ColumnTransformer([
            ('num', numerical_pipeline, numerical_cols),
            ('cat', encoder, categorical_cols)
        ], remainder='drop')

        self.preprocessor = full_preprocessor.fit(self.df_raw)
        self.df_processed = pd.DataFrame(self.preprocessor.transform(self.df_raw))
        return self.df_processed

    def save_processed(self, output_path):
        self.df_processed.to_csv(output_path, index=False)
        print(f"âœ… Processed data saved at: {output_path}")
''')

print("âœ… All Python modules written successfully.")

import pandas as pd
import numpy as np
import os
from datetime import datetime, timedelta

np.random.seed(42)
n_samples = 200

data = {
    "customer_id": range(1, n_samples + 1),
    "age": np.random.randint(18, 70, n_samples),
    "income": np.round(np.random.normal(50000, 15000, n_samples), 2),
    "gender": np.random.choice(["Male", "Female", None], n_samples, p=[0.45, 0.45, 0.1]),
    "subscription_date": [(datetime.now() - timedelta(days=np.random.randint(1, 1000))).strftime("%Y-%m-%d") for _ in range(n_samples)],
    "product_category": np.random.choice(["Electronics", "Clothing", "Home", "Books", None], n_samples),
    "purchase_amount": np.round(np.random.exponential(200, n_samples), 2),
    "is_churned": np.random.choice([0, 1], n_samples, p=[0.7, 0.3])
}

df = pd.DataFrame(data)

# Introduce some NaNs manually
df.loc[[5, 10, 15], 'income'] = np.nan
df.loc[[8, 12], 'product_category'] = np.nan

# Save to CSV
output_path = f'{project_root}/data/raw/synthetic_data.csv'
df.to_csv(output_path, index=False)
print(f"âœ… Synthetic dataset saved at: {output_path}")

# from autoai_insight.backend.data_pipeline.preprocessing import DataPreprocessor # Remove this line
from backend.data_pipeline.preprocessing import DataPreprocessor # Add this line

config = {
    'numerical_strategy': 'median',
    'categorical_strategy': 'mode',
    'scale_numerical': True,
    'encode_categorical': True,
    'detect_outliers': True
}

preprocessor = DataPreprocessor(config)
cleaned_df = preprocessor.fit_transform(f"{project_root}/data/raw/synthetic_data.csv")
preprocessor.save_processed(f"{project_root}/data/processed/cleaned_data.csv")

print("ðŸ“Š First 5 rows of cleaned data:")
print(cleaned_df.head())

#EDA Notebook

import os
from google.colab import files

#define project root
project_root='/content/autoai-insight'

os.makedirs(f"{project_root}/reports", exist_ok=True)

notebook_content='''{
  "cells":[
    {
      "cell_type":"markdown",
      "metadata":{},
      "source":[
        "#Auto AI Insight: Explanatory Data Analysis"
      ]
    },
    {
      "cell_type":"code",
      "execution_count":null,
      "metadata":{},
      "outputs":[],
      "source":[
        "import pandas as pd\n import numpy as np \n import seaborn as sns \n import matplotlib.pyplot as plt\n
        from IPython.display import display, HTML \n import os \n\n %matplotlib inline \n plt.style.use('seaborn')\n\n
        # Load dataset\n project_root='content/autoai-insight'\n data_path=os.path.join(project_root,'data','raw','synthetic_data.csv')\n
        df = pd.read_csv(data_path)\n\n print(\"Dataset loaded successfully!\")"
      ]
    },
    {
      "cell_type":"markdown",
      "metadata":{},
      "source":[
        "## Feature Type Overview"
      ]
    },
    {
      "cell_type":"code",
      "execution_count":null,
      "metadata":{},
      "outputs":[],
      "source":[
        "def detect_column_types(df):
          type_into={}
          for col in df.columns:
            if pd.api.types.is_numeric_dtype(df[col]):
              type_info[col]='numerical'
            elif pd.api.types.is_datetime64_any_dtype(df[col]):
              type_info[col]='datetime
            else:
              unique_ratio=df[col].nunique()/len(df[col])
              if unique_ratio<0.05:
                type_info[col]='categorical'
              else:
                type_info[col]='text'
          return type_info

        types= detect_column_types(df)
        #Display types
        type_df=pd.DataFrame.from_dict(types,orient='index',columns=['Type])
        print("Feature Types:")
        display(type_df)"
      ]
    },
    {
    "cell_type":"markdown",
    "metadata":{},
    "source":[
      "## Univariate Analysis"
    ]},
    {
      "cell_type":"markdown",
      "metadata":{},
      "source":[
        "## Correlation analysis"
      ]
    },
    {
      "cell_type":"code",
      "execution_count":null,
      "metadata":{},
      "outputs":[],
      "source":[
        "numeric_df= df.select_dtypes(include=np.number)
        if not numeric_df.empty:
          plt.figure(figsize=(10,8))
          sns.heatmap(numeric_df.corr(),annot=True,cmap="coolwarm",fmt'.2f')
          plt.title("Feature correlation Heatmap")
          plt.show()
        else:
          print("No numerical featuirs to compute correlation.")"
      ]
    },
    {
      "cell_type":"markdown",
      "metadata":{},
      "source":[
        "Target Analysis (if target column exists)"
      ]
    },
    {
      "cell_type":"code":
      "execution_count":null,
      "metadata":{},
      "outputs":[],
      "source":[
        "target_col='is_churned'
        if target_col in df.columns:
        for col in df.columns:
          if col !=target_col:
            if pd.api.types.is_numeric_dtype(df[col]):
              plt.figure(figsize=(8,4))
              sns.violinplot(x=df[target_col],y=df[col])
              plt.title(f"{col} vs {target_col}")
              plt.show()
            elif types[col]== 'categorical':
              grouped=df.groupby(col)[target_col].mean().sort_values()
              plt.figure(figsize=(8,4))
              grouped.plot(kind='bar')
              plt.title(f"{col} vs {target_col}{Average}')
              plt.xticks(rotation=45)
              plt.ylabel(target_col)
              plt.show()
      ]
    },
    {
      "cell_types":"markdown",
      "metadata":{},
      "source":[
        "## Save auto generated EDA report"
      ]
    },
    {
      "cell_type":"code",
      "execution_count":null,
      "metadata":{},
      "outputs":[],
      "source":[
        "from nbconvert import HTMLExporter
        import nbformat
        import codecs
        import os
        notebook_path='/content/eda.ipynb'
        output_path=os.path.join(project_root,'reports','eda_report.html')
        with open(notebook_path,'r',encoding='utf-8') as fh:
          nb=nbformat.read(fh, as_version=4)

        html_exporter=HTMLExporter()
        html_exporter.template_file='classic'
        (body,resources)=html_exporter.from_notebook_node(nb)
        with codecs.open(output_path,'w',encoding='utf-8') as f:
          f.write(body)
        print(f"EDA report saved at: {output_path}")"
      ]
    },
    {
      "metadata":{
        "kernelspec":{
          "display_name":"Python 3",
          "language":"python",
          "name":python3
        },
        "language_info":{
          "codemirror_mode":{
            "name":"ipython",
            "version":3
          },
          "file_extension":".py",
          "mimetype":"text/x-python",
          "name":"python",
          "nbconvert_exporter":"python",
          "pygments_lexer":"ipython3",
          "version":"3.9"
        }
      },
      "nbformat":4,
      "nbformat_minor":4
    }
'''

notebook_path='/content/eda.ipynb'
with open (notebook_path,'w') as f:
  f.write(notebook_content)

print("EDA notebook created: eda.ipynb")
files.download('eda.ipynb')
print("File downloaded to your machine")

